{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Strategy\n",
    "\n",
    "* I decided to go with webscrapping of the website magicbricks.com to get information of different realestate in mumbai.\n",
    "* The collection of the data is done using Selenium which uses chromedriver to access my chrome browser. It will then scroll down on the page to get the next set of records.\n",
    "* This method of collection was implemented because URLlib was being denied access to the website and accessing the website through a browser was the only way.\n",
    "* After collecting the data I have created a csv file for the same and stored it locally on my machine.\n",
    "\n",
    "##### Challenges faced\n",
    "\n",
    "* Execution time for scrapping from selenium is acceptable. For extracting 210 records it takes 43 seconds.\n",
    "* Extraction of BHK,advertiser and price was easy since it had its own seperate class respectively.\n",
    "* Other parameters came under a single class which was a card summary on the page for each property.\n",
    "* Finding all classes for each records takes some time. However converting these records into a numpy array is a better strategy since arrays are faster and occupy lesser memory space. \n",
    "* Posted Date was present under another class name so I was not able to use the same function to extract the card parameters. I also had to tackle certain records that had yesterday and today instead of the date. Used date function to tackle this problem.\n",
    "* There were some records that had \"for sale\" before location and some had \"for sale in\" before location. Luckily there were only these 2 versions of this value in each card_record. So I had to write a seperate loop for location to check for both \"for sale\" and \"for sale in\".\n",
    "* Lastly I wrote code that would concat any new records to the previous csv created so that my database keeps increasing.\n",
    "* Since I have written code for concatinating, we can automate this script to run everyweek or everyday and capture 50-100 records per day to get latest records added into are dataset.\n",
    "* I would like to store this information in an S3 bucket on amazon aws or any other cloud based solution that is of least cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen as uReq\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL from where we are fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'Mumbai'\n",
    "# url = 'https://www.magicbricks.com/property-for-sale/residential-real-estate?proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment&cityName='+loc\n",
    "url = 'https://www.magicbricks.com/property-for-sale/residential-real-estate?proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment&cityName=Mumbai&BudgetMin=30-Lacs&BudgetMax=50-Lacs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping the data using Selenium library and chromedriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 25.018433332443237\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome('Drivers/chromedriver.exe')\n",
    "\n",
    "driver.get(url)\n",
    "# driver.maximize_window()\n",
    "for i in range(15): #Running the loop for 50 instances. We roughly get 10 records for each instance but this varies due to the page structure.\n",
    "    \n",
    "    driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\") #This code is responsible for scrolling the page down so that more records are populated.\n",
    "    time.sleep(0.6) #A sleep time is given to prevent magic bricks from locking us out due to server overload.\n",
    "end = time.time()\n",
    "print(\"Execution time:\",end-start) #To calculate time for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 29.02517557144165\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "page_html = bs(driver.page_source,\"html.parser\") #passing the page_source from driver object into an html parser.\n",
    "end = time.time()\n",
    "print(\"Execution time:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First level of extraction of data from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3.3217790126800537\n"
     ]
    }
   ],
   "source": [
    "s1 = time.time()\n",
    "\n",
    "#Extracting values from the html using findAll and class name. For example in the 1st case inside span we have that class name mentioned on the page.\n",
    "price_html = page_html.findAll('div',{'class':'m-srp-card__price'})\n",
    "date_created = page_html.findAll('span',{'itemprop':'dateCreated'})\n",
    "card = np.array(page_html.findAll('div',{'class':'m-srp-card__desc flex__item'}))\n",
    "\n",
    "# cleaning the data in card recordsto have only values and no empty values in the list\n",
    "card_records = []\n",
    "for j in range(len(card)):\n",
    "    card_records.append([i for i in card[j].text.split('\\n') if i!=''])\n",
    "s2 = time.time()\n",
    "print('Execution time:',s2-s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the data from the card_records variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many records have the terms for Sale and for Sale in?\n",
    "check = 0\n",
    "for i in card_records:\n",
    "    if 'for Sale' in i or 'for Sale in' in i:\n",
    "        check+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can get the location info for each record since the next 2 values after for Sale in the list are location\n",
    "location = []\n",
    "for i in card_records:\n",
    "    if 'for Sale' in i:\n",
    "        location.append(i[i.index('for Sale')+1:i.index('for Sale')+3])\n",
    "    elif 'for Sale in' in i:\n",
    "        location.append(i[i.index('for Sale in')+1:i.index('for Sale in')+3])\n",
    "location_series = pd.Series(location)\n",
    "len(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_impute(records,search_term1,index_next,ifnot_term = None):\n",
    "    location = []\n",
    "    try:\n",
    "        for i in records:\n",
    "            if search_term1 in i:\n",
    "                location.append(i[i.index(search_term1)+index_next])\n",
    "            else:\n",
    "                location.append(ifnot_term)\n",
    "    except:\n",
    "        location.append(np.nan)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the above created function to extract all avaiable features in card_records variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for super area and carpet area. We will make manual changes later\n",
    "area_calc_on = pd.Series(find_and_impute(records = card_records, search_term1 = 'carpet area',index_next = 0,ifnot_term = 'super area'))\n",
    "#Extract the status of each property. If unavailable then imputing nan\n",
    "status = pd.Series(find_and_impute(card_records,search_term1 = 'status',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting the floor of each property. If unavailable then imputing nan\n",
    "floors = pd.Series(find_and_impute(card_records,search_term1 = 'floor',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting the transaction type for each property. New or resale.\n",
    "new_or_resale = pd.Series(find_and_impute(card_records,search_term1 = 'transaction',index_next = 1,ifnot_term = np.nan))\n",
    "# Extracting type of furnishing.\n",
    "furnishing = pd.Series(find_and_impute(card_records,search_term1 = 'furnishing',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting Society\n",
    "society = pd.Series(find_and_impute(card_records,search_term1 = 'society',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting number of bathrooms on property\n",
    "bathrooms = pd.Series(find_and_impute(card_records,search_term1 = 'bathroom',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting the builder name\n",
    "advertiser_name = pd.Series([i[-1] for i in card_records])\n",
    "#Extracting BHK\n",
    "bhk = pd.Series([i[0] for i in card_records])\n",
    "#Extracting the price from price_html\n",
    "price = pd.Series([i.text for i in price_html])\n",
    "#Extract parking\n",
    "parking = pd.Series(find_and_impute(card_records,search_term1 = 'car parking',index_next = 1,ifnot_term = np.nan))\n",
    "#Extract comments\n",
    "comments = pd.Series(find_and_impute(card_records,search_term1 = 'read more',index_next = -1,ifnot_term = np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the date posted\n",
    "date_txt = [i.text for i in date_created]\n",
    "today = date.today()\n",
    "yesterday = today - timedelta(days = 1) \n",
    "date_vals = []\n",
    "for i in date_txt:\n",
    "    if i.lower() == 'today':\n",
    "        date_vals.append(today.strftime(\"%b\")+\" \"+today.strftime(\"%d\")+\",\"+today.strftime(\" '%y\"))\n",
    "    elif i.lower() == 'yesterday':\n",
    "        date_vals.append(yesterday.strftime(\"%b\")+\" \"+yesterday.strftime(\"%d\")+\",\"+yesterday.strftime(\" '%y\"))\n",
    "    else:\n",
    "        date_vals.append(i)\n",
    "date_series = pd.Series(date_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Area values,facing,overlooking\n",
    "#Find facing\n",
    "facing = pd.Series(find_and_impute(card_records,search_term1 = 'facing',index_next = 1,ifnot_term = np.nan))\n",
    "#Find overlooking\n",
    "overlooking = pd.Series(find_and_impute(card_records,search_term1 = 'overlooking',index_next = 1,ifnot_term = np.nan))\n",
    "#Area values\n",
    "area_values = []\n",
    "for i in card_records:\n",
    "    if 'carpet area' in i:\n",
    "        area_values.append(i[i.index('carpet area')+1])\n",
    "    elif 'super area' in i:\n",
    "        area_values.append(i[i.index('super area')+1])\n",
    "area = pd.Series(area_values)\n",
    "len(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((area_calc_on,status,floors,new_or_resale,furnishing,society,\n",
    "                bathrooms,advertiser_name,bhk,price,parking,comments,\n",
    "                location_series,date_series,facing,overlooking,area),axis = 1)\n",
    "\n",
    "n = ['Area_Type','Possession_Status','Floors','new_or_resale',\n",
    "     'furnishing','society','bathrooms','advertiser_name','bhk',\n",
    "     'price','parking','comments','location','date_of_posting',\n",
    "     'facing','overlooking','area']\n",
    "\n",
    "names = dict(zip(list(range(len(n))),n))\n",
    "df_final = df.rename(names,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 390 entries, 0 to 389\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Area_Type          390 non-null    object\n",
      " 1   Possession_Status  386 non-null    object\n",
      " 2   Floors             381 non-null    object\n",
      " 3   new_or_resale      390 non-null    object\n",
      " 4   furnishing         389 non-null    object\n",
      " 5   society            206 non-null    object\n",
      " 6   bathrooms          387 non-null    object\n",
      " 7   advertiser_name    390 non-null    object\n",
      " 8   bhk                390 non-null    object\n",
      " 9   price              390 non-null    object\n",
      " 10  parking            137 non-null    object\n",
      " 11  comments           222 non-null    object\n",
      " 12  location           390 non-null    object\n",
      " 13  date_of_posting    390 non-null    object\n",
      " 14  facing             295 non-null    object\n",
      " 15  overlooking        275 non-null    object\n",
      " 16  area               390 non-null    object\n",
      "dtypes: object(17)\n",
      "memory usage: 51.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(os.listdir('Data'))<=1:\n",
    "    df_final.to_csv('Data/scrapped.csv',index = False)\n",
    "else:\n",
    "    new_data = df_final\n",
    "    old_data = pd.read_csv('Data/scrappedv'+str(len(os.listdir('Data'))-1)+'.csv')\n",
    "    new_final_data = pd.concat((new_data,old_data)).reset_index(drop = True)\n",
    "    new_final_data.to_csv('Data/scrappedv'+str(len(os.listdir('Data')))+'.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2561, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area_Type</th>\n",
       "      <th>Possession_Status</th>\n",
       "      <th>Floors</th>\n",
       "      <th>new_or_resale</th>\n",
       "      <th>furnishing</th>\n",
       "      <th>society</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>advertiser_name</th>\n",
       "      <th>bhk</th>\n",
       "      <th>price</th>\n",
       "      <th>parking</th>\n",
       "      <th>comments</th>\n",
       "      <th>location</th>\n",
       "      <th>date_of_posting</th>\n",
       "      <th>facing</th>\n",
       "      <th>overlooking</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>carpet area</td>\n",
       "      <td>Ready to Move</td>\n",
       "      <td>2 out of 11 floors</td>\n",
       "      <td>New Property</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>Jyoti Breeze</td>\n",
       "      <td>2.0</td>\n",
       "      <td>View Agent Profile</td>\n",
       "      <td>1 BHK Apartment</td>\n",
       "      <td>₹ 48 Lac</td>\n",
       "      <td>1 Open</td>\n",
       "      <td>Venkatesh Jyoti Breeze is an premium Project l...</td>\n",
       "      <td>['Jyoti Breeze,', 'Mira Road']</td>\n",
       "      <td>Feb 04, '21</td>\n",
       "      <td>East</td>\n",
       "      <td>Garden/Park, Main Road</td>\n",
       "      <td>480 sqft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>carpet area</td>\n",
       "      <td>Possession by Jan '22</td>\n",
       "      <td>5 out of 14 floors</td>\n",
       "      <td>New Property</td>\n",
       "      <td>Semi-Furnished</td>\n",
       "      <td>Agarwal Paramount</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100+ Buyers Served</td>\n",
       "      <td>2 BHK Apartment</td>\n",
       "      <td>₹ 47 Lac</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Agarwal Paramount,', 'Virar West']</td>\n",
       "      <td>Feb 05, '21</td>\n",
       "      <td>East</td>\n",
       "      <td>Garden/Park, Main Road</td>\n",
       "      <td>604 sqft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area_Type      Possession_Status              Floors new_or_resale  \\\n",
       "118  carpet area          Ready to Move  2 out of 11 floors  New Property   \n",
       "46   carpet area  Possession by Jan '22  5 out of 14 floors  New Property   \n",
       "\n",
       "         furnishing            society  bathrooms     advertiser_name  \\\n",
       "118     Unfurnished       Jyoti Breeze        2.0  View Agent Profile   \n",
       "46   Semi-Furnished  Agarwal Paramount        2.0  100+ Buyers Served   \n",
       "\n",
       "                 bhk     price parking  \\\n",
       "118  1 BHK Apartment  ₹ 48 Lac  1 Open   \n",
       "46   2 BHK Apartment  ₹ 47 Lac     NaN   \n",
       "\n",
       "                                              comments  \\\n",
       "118  Venkatesh Jyoti Breeze is an premium Project l...   \n",
       "46                                                 NaN   \n",
       "\n",
       "                                 location date_of_posting facing  \\\n",
       "118        ['Jyoti Breeze,', 'Mira Road']     Feb 04, '21   East   \n",
       "46   ['Agarwal Paramount,', 'Virar West']     Feb 05, '21   East   \n",
       "\n",
       "                overlooking      area  \n",
       "118  Garden/Park, Main Road  480 sqft  \n",
       "46   Garden/Park, Main Road  604 sqft  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pd.read_csv('Data/scrappedv'+str(len(os.listdir('Data'))-1)+'.csv').shape)\n",
    "pd.read_csv('Data/scrappedv'+str(len(os.listdir('Data'))-1)+'.csv').sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
