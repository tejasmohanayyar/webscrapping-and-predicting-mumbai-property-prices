{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Strategy\n",
    "\n",
    "* I decided to go with webscrapping of the website magicbricks.com to get information of different realestate in mumbai.\n",
    "* The collection of the data is done using Selenium which uses chromedriver to access my chrome browser. It will then scroll down on the page to get the next set of records.\n",
    "* This method of collection was implemented because URLlib was being denied access to the website and accessing the website through a browser was the only way.\n",
    "* After collecting the data I have created a csv file for the same and stored it locally on my machine.\n",
    "\n",
    "##### Challenges faced\n",
    "\n",
    "* Execution time for scrapping from selenium is acceptable. For extracting 210 records it takes 43 seconds.\n",
    "* Extraction of BHK,advertiser and price was easy since it had its own seperate class respectively.\n",
    "* Other parameters came under a single class which was a card summary on the page for each property.\n",
    "* Finding all classes for each records takes some time. However converting these records into a numpy array is a better strategy since arrays are faster and occupy lesser memory space. \n",
    "* Posted Date was present under another class name so I was not able to use the same function to extract the card parameters. I also had to tackle certain records that had yesterday and today instead of the date. Used date function to tackle this problem.\n",
    "* There were some records that had \"for sale\" before location and some had \"for sale in\" before location. Luckily there were only these 2 versions of this value in each card_record. So I had to write a seperate loop for location to check for both \"for sale\" and \"for sale in\".\n",
    "* Lastly I wrote code that would concat any new records to the previous csv created so that my database keeps increasing.\n",
    "* Since I have written code for concatinating, we can automate this script to run everyweek or everyday and capture 50-100 records per day to get latest records added into are dataset.\n",
    "* I would like to store this information in an S3 bucket on amazon aws or any other cloud based solution that is of least cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen as uReq\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL from where we are fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'Mumbai'\n",
    "url = 'https://www.magicbricks.com/property-for-sale/residential-real-estate?proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment&cityName='+loc\n",
    "# url = 'https://www.magicbricks.com/property-for-sale/residential-real-estate?bedroom=2&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment&Locality=Colaba&cityName=Mumbai'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping the data using Selenium library and chromedriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 14.221520185470581\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome('Drivers/chromedriver.exe')\n",
    "\n",
    "driver.get(url)\n",
    "# driver.maximize_window()\n",
    "for i in range(15): #Running the loop for 50 instances. We roughly get 10 records for each instance but this varies due to the page structure.\n",
    "    \n",
    "    driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\") #This code is responsible for scrolling the page down so that more records are populated.\n",
    "    time.sleep(0.6) #A sleep time is given to prevent magic bricks from locking us out due to server overload.\n",
    "end = time.time()\n",
    "print(\"Execution time:\",end-start) #To calculate time for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2.4687836170196533\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "page_html = bs(driver.page_source,\"html.parser\") #passing the page_source from driver object into an html parser.\n",
    "end = time.time()\n",
    "print(\"Execution time:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First level of extraction of data from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.4169192314147949\n"
     ]
    }
   ],
   "source": [
    "s1 = time.time()\n",
    "\n",
    "#Extracting values from the html using findAll and class name. For example in the 1st case inside span we have that class name mentioned on the page.\n",
    "price_html = page_html.findAll('div',{'class':'m-srp-card__price'})\n",
    "date_created = page_html.findAll('span',{'itemprop':'dateCreated'})\n",
    "card = np.array(page_html.findAll('div',{'class':'m-srp-card__desc flex__item'}))\n",
    "\n",
    "# cleaning the data in card recordsto have only values and no empty values in the list\n",
    "card_records = []\n",
    "for j in range(len(card)):\n",
    "    card_records.append([i for i in card[j].text.split('\\n') if i!=''])\n",
    "s2 = time.time()\n",
    "print('Execution time:',s2-s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the data from the card_records variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many records have the terms for Sale and for Sale in?\n",
    "check = 0\n",
    "for i in card_records:\n",
    "    if 'for Sale' in i or 'for Sale in' in i:\n",
    "        check+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can get the location info for each record since the next 2 values after for Sale in the list are location\n",
    "location = []\n",
    "for i in card_records:\n",
    "    if 'for Sale' in i:\n",
    "        location.append(i[i.index('for Sale')+1:i.index('for Sale')+3])\n",
    "    elif 'for Sale in' in i:\n",
    "        location.append(i[i.index('for Sale in')+1:i.index('for Sale in')+3])\n",
    "location_series = pd.Series(location)\n",
    "len(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_impute(records,search_term1,index_next,ifnot_term = None):\n",
    "    location = []\n",
    "    try:\n",
    "        for i in records:\n",
    "            if search_term1 in i:\n",
    "                location.append(i[i.index(search_term1)+index_next])\n",
    "            else:\n",
    "                location.append(ifnot_term)\n",
    "    except:\n",
    "        location.append(np.nan)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the above created function to extract all avaiable features in card_records variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for super area and carpet area. We will make manual changes later\n",
    "area_calc_on = pd.Series(find_and_impute(records = card_records, search_term1 = 'carpet area',index_next = 0,ifnot_term = 'super area'))\n",
    "#Extract the status of each property. If unavailable then imputing nan\n",
    "status = pd.Series(find_and_impute(card_records,search_term1 = 'status',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting the floor of each property. If unavailable then imputing nan\n",
    "floors = pd.Series(find_and_impute(card_records,search_term1 = 'floor',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting the transaction type for each property. New or resale.\n",
    "new_or_resale = pd.Series(find_and_impute(card_records,search_term1 = 'transaction',index_next = 1,ifnot_term = np.nan))\n",
    "# Extracting type of furnishing.\n",
    "furnishing = pd.Series(find_and_impute(card_records,search_term1 = 'furnishing',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting Society\n",
    "society = pd.Series(find_and_impute(card_records,search_term1 = 'society',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting number of bathrooms on property\n",
    "bathrooms = pd.Series(find_and_impute(card_records,search_term1 = 'bathroom',index_next = 1,ifnot_term = np.nan))\n",
    "#Extracting the builder name\n",
    "advertiser_name = pd.Series([i[-1] for i in card_records])\n",
    "#Extracting BHK\n",
    "bhk = pd.Series([i[0] for i in card_records])\n",
    "#Extracting the price from price_html\n",
    "price = pd.Series([i.text for i in price_html])\n",
    "#Extract parking\n",
    "parking = pd.Series(find_and_impute(card_records,search_term1 = 'car parking',index_next = 1,ifnot_term = np.nan))\n",
    "#Extract comments\n",
    "comments = pd.Series(find_and_impute(card_records,search_term1 = 'read more',index_next = -1,ifnot_term = np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the date posted\n",
    "date_txt = [i.text for i in date_created]\n",
    "today = date.today()\n",
    "yesterday = today - timedelta(days = 1) \n",
    "date_vals = []\n",
    "for i in date_txt:\n",
    "    if i.lower() == 'today':\n",
    "        date_vals.append(today.strftime(\"%b\")+\" \"+today.strftime(\"%d\")+\",\"+today.strftime(\" '%y\"))\n",
    "    elif i.lower() == 'yesterday':\n",
    "        date_vals.append(yesterday.strftime(\"%b\")+\" \"+yesterday.strftime(\"%d\")+\",\"+yesterday.strftime(\" '%y\"))\n",
    "    else:\n",
    "        date_vals.append(i)\n",
    "date_series = pd.Series(date_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Area values,facing,overlooking\n",
    "#Find facing\n",
    "facing = pd.Series(find_and_impute(card_records,search_term1 = 'facing',index_next = 1,ifnot_term = np.nan))\n",
    "#Find overlooking\n",
    "overlooking = pd.Series(find_and_impute(card_records,search_term1 = 'overlooking',index_next = 1,ifnot_term = np.nan))\n",
    "#Area values\n",
    "area_values = []\n",
    "for i in card_records:\n",
    "    if 'carpet area' in i:\n",
    "        area_values.append(i[i.index('carpet area')+1])\n",
    "    elif 'super area' in i:\n",
    "        area_values.append(i[i.index('super area')+1])\n",
    "area = pd.Series(area_values)\n",
    "len(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((area_calc_on,status,floors,new_or_resale,furnishing,society,\n",
    "                bathrooms,advertiser_name,bhk,price,parking,comments,\n",
    "                location_series,date_series,facing,overlooking,area),axis = 1)\n",
    "\n",
    "n = ['Area_Type','Possession_Status','Floors','new_or_resale',\n",
    "     'furnishing','society','bathrooms','advertiser_name','bhk',\n",
    "     'price','parking','comments','location','date_of_posting',\n",
    "     'facing','overlooking','area']\n",
    "\n",
    "names = dict(zip(list(range(len(n))),n))\n",
    "df_final = df.rename(names,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41 entries, 0 to 40\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Area_Type          41 non-null     object\n",
      " 1   Possession_Status  41 non-null     object\n",
      " 2   Floors             39 non-null     object\n",
      " 3   new_or_resale      41 non-null     object\n",
      " 4   furnishing         41 non-null     object\n",
      " 5   society            12 non-null     object\n",
      " 6   bathrooms          41 non-null     object\n",
      " 7   advertiser_name    41 non-null     object\n",
      " 8   bhk                41 non-null     object\n",
      " 9   price              41 non-null     object\n",
      " 10  parking            15 non-null     object\n",
      " 11  comments           23 non-null     object\n",
      " 12  location           41 non-null     object\n",
      " 13  date_of_posting    41 non-null     object\n",
      " 14  facing             26 non-null     object\n",
      " 15  overlooking        20 non-null     object\n",
      " 16  area               41 non-null     object\n",
      "dtypes: object(17)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(os.listdir('Data'))<=1:\n",
    "    df_final.to_csv('Data/scrapped.csv',index = False)\n",
    "else:\n",
    "    new_data = df_final\n",
    "    old_data = pd.read_csv('Data/scrappedv'+str(len(os.listdir('Data'))-1)+'.csv')\n",
    "    new_final_data = pd.concat((new_data,old_data)).reset_index(drop = True)\n",
    "    new_final_data.to_csv('Data/scrappedv'+str(len(os.listdir('Data')))+'.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2171, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area_Type</th>\n",
       "      <th>Possession_Status</th>\n",
       "      <th>Floors</th>\n",
       "      <th>new_or_resale</th>\n",
       "      <th>furnishing</th>\n",
       "      <th>society</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>advertiser_name</th>\n",
       "      <th>bhk</th>\n",
       "      <th>price</th>\n",
       "      <th>parking</th>\n",
       "      <th>comments</th>\n",
       "      <th>location</th>\n",
       "      <th>date_of_posting</th>\n",
       "      <th>facing</th>\n",
       "      <th>overlooking</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>carpet area</td>\n",
       "      <td>Possession by Dec '21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Property</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>Avant Heritage 2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Avant Group</td>\n",
       "      <td>2 BHK Flat</td>\n",
       "      <td>₹ 1.32 Cr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Avant Heritage presents an extravagant luxury ...</td>\n",
       "      <td>['Avant Heritage 2,', 'in Jogeshwari East']</td>\n",
       "      <td>Jan 27, '21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>590 sqft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>carpet area</td>\n",
       "      <td>Possession by May '21</td>\n",
       "      <td>7 out of 20 floors</td>\n",
       "      <td>New Property</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>Bhimashankar Heights</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500+ Buyers Served</td>\n",
       "      <td>2 BHK Apartment</td>\n",
       "      <td>₹ 1.51 Cr</td>\n",
       "      <td>1 Covered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Bhimashankar Heights,', 'Kandarpada']</td>\n",
       "      <td>Jan 30, '21</td>\n",
       "      <td>North - East</td>\n",
       "      <td>Garden/Park, Main Road</td>\n",
       "      <td>734 sqft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Area_Type      Possession_Status              Floors new_or_resale  \\\n",
       "1265  carpet area  Possession by Dec '21                 NaN  New Property   \n",
       "382   carpet area  Possession by May '21  7 out of 20 floors  New Property   \n",
       "\n",
       "       furnishing               society  bathrooms      advertiser_name  \\\n",
       "1265  Unfurnished      Avant Heritage 2        2.0          Avant Group   \n",
       "382   Unfurnished  Bhimashankar Heights        2.0  1500+ Buyers Served   \n",
       "\n",
       "                  bhk      price    parking  \\\n",
       "1265       2 BHK Flat  ₹ 1.32 Cr        NaN   \n",
       "382   2 BHK Apartment  ₹ 1.51 Cr  1 Covered   \n",
       "\n",
       "                                               comments  \\\n",
       "1265  Avant Heritage presents an extravagant luxury ...   \n",
       "382                                                 NaN   \n",
       "\n",
       "                                         location date_of_posting  \\\n",
       "1265  ['Avant Heritage 2,', 'in Jogeshwari East']     Jan 27, '21   \n",
       "382       ['Bhimashankar Heights,', 'Kandarpada']     Jan 30, '21   \n",
       "\n",
       "            facing             overlooking      area  \n",
       "1265           NaN                     NaN  590 sqft  \n",
       "382   North - East  Garden/Park, Main Road  734 sqft  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pd.read_csv('Data/scrappedv'+str(len(os.listdir('Data'))-1)+'.csv').shape)\n",
    "pd.read_csv('Data/scrappedv'+str(len(os.listdir('Data'))-1)+'.csv').sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
